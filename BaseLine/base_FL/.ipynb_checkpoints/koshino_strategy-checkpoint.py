from collections import OrderedDict
from typing import List, Tuple, Optional, Dict, Union, Callable
import matplotlib.pyplot as plt
import numpy as np
from datasets.utils.logging import enable_progress_bar
enable_progress_bar()
import time
import math
import os
import pandas as pd
from IPython.display import display
os.environ["RAY_DEDUP_LOGS"] = "0"
# os.environ['CUDA_VISIBLE_DEVICES'] = "1"
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
from sklearn.cluster import AgglomerativeClustering

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torchvision.models as models
from torch.amp import autocast, GradScaler  # âœ… AMP æ··åˆç²¾åº¦
torch.backends.cudnn.benchmark = True

import flwr
from flwr.common import EvaluateIns, EvaluateRes, FitIns, FitRes, Parameters, Scalar, ndarrays_to_parameters, parameters_to_ndarrays , NDArrays, Context, Metrics, MetricsAggregationFn    
from flwr.server.strategy import FedAvg
from flwr.simulation import run_simulation
from flwr.simulation import start_simulation
from flwr.server.client_manager import ClientManager
from flwr.server.client_proxy import ClientProxy

from koshino_FL.koshino_model_CNN import Net
# from koshino_FL.airbench94_muon import CifarNet as Net
# from koshino_FL.koshino_model_CNN import MobileNetV3_Small as Net
from koshino_FL.koshino_train_v6 import test
from koshino_FL.koshino_others import set_parameters
from koshino_FL.koshino_Similarity_Measurement import  _compute_similarity_compressed
from koshino_FL.koshino_loaddata import get_cached_datasets
from koshino_FL.koshino_history import history

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Training on {DEVICE}")
print(f"Flower {flwr.__version__} / PyTorch {torch.__version__}")
# disable_progress_bar()


def check_and_clean_params(
    params: List[np.ndarray],
    cid: str
) -> List[np.ndarray]:
    """è‹¥åƒæ•¸å«æœ‰ NaN/Infï¼Œåšä¸€äº›ç°¡å–®è™•ç†ï¼ˆä¾‹å¦‚ç›´æ¥è¨­ç‚º0ï¼‰."""
    cleaned_params = []
    for idx, w in enumerate(params):
        if np.isnan(w).any() or np.isinf(w).any():
            print(f"[Warning] Client {cid}, layer {idx} has NaN/Inf - replacing with zeros.")
            w = np.nan_to_num(w, nan=0.0, posinf=0.0, neginf=0.0)
        cleaned_params.append(w)
    return cleaned_params

def parameters_to_vector(numpy_params: List[np.ndarray]) -> torch.Tensor:
    flat_list = []
    for p in numpy_params:
        flat_list.append(p.reshape(-1))  # æ”¤å¹³
    merged = np.concatenate(flat_list, axis=0)
    return torch.from_numpy(merged)

def gradient_of(
    client_params: List[np.ndarray],
    global_params: List[np.ndarray]
) -> List[np.ndarray]:
    return [cp - gp for cp, gp in zip(client_params, global_params)]

def init_or_check_client_info_table(
    client_info_table: Dict[str, Dict[str, float]],
    client_ids: List[str],
) -> None:
    """
    æª¢æŸ¥ client_info_table æ˜¯å¦å·²æœ‰å°æ‡‰çš„ client idï¼Œ
    è‹¥æ²’æœ‰å‰‡åˆå§‹åŒ–è©² client åœ¨è¡¨è£¡çš„ç´€éŒ„ã€‚
    """
    for cid in client_ids:
        if cid not in client_info_table:
            # åˆå§‹åŒ– reputation, last_loss ç­‰ç­‰
            client_info_table[cid] = {
                "partition_id": -1,
                "client_accuracy": 0.0,
                "is_suspicious": False,
                "reputation": 0.3,        # ç¶œåˆè²è­½ï¼ˆå¯æ”¹ç‚ºé•·æœŸè²è­½ï¼‰
                "short_term_rs": 0.3,     # æ–°å¢ï¼šçŸ­æœŸRS
                "long_term_rs": 0.3,      # æ–°å¢ï¼šé•·æœŸRS
                "client_loss": 9999.9,
                "loss_history": [],       # æ–°å¢ï¼šç”¨æ–¼è¨ˆç®—æ³¢å‹•åº¦
                "similarity": -1.0,
                "similarity_history": [],  # æ–°å¢ï¼šç”¨æ–¼é•·æœŸå¯é æ€§
                "shapley": 0.0,
                "shapley_history": [],
            }

def re_aggregate_without(
    client_params_dict: Dict[str, List[np.ndarray]],
    exclude_cid: str,
    client_info_table: Dict[str, Dict[str, float]],
    beta: float = 0.7,
) -> Optional[List[np.ndarray]]:
    """
    é‡æ–°èšåˆ(åŠ æ¬Š)æ‰€æœ‰ client è£¡ï¼Œæ’é™¤ exclude_cid çš„åƒæ•¸ã€‚
    å›å‚³è©²ã€Œå»æ‰æŸäººã€å¾Œçš„èšåˆæ¨¡å‹åƒæ•¸ã€‚
    """
    # 1. éæ¿¾æ‰æƒ³è¦æ’é™¤çš„ client
    partial_clients = {
        cid: params for cid, params in client_params_dict.items() 
        if cid != exclude_cid
    }
    if not partial_clients:
        # æ„å‘³è‘—åªæœ‰é€™å€‹ cid ä¸€å€‹å®¢æˆ¶ç«¯åœ¨å‚³åƒæ•¸ï¼Œç§»é™¤å¾Œå°±ç©ºäº†
        return None
    
    # 2. åš NaN/Inf æ¸…æ´—
    for cid, params in partial_clients.items():
        partial_clients[cid] = check_and_clean_params(params, cid)
    
    # 3. æ ¹æ“š short_term_rs/long_term_rs åšåŠ æ¬Šèšåˆ
    weighted_params = None
    total_weight = 0.0
    for cid, params in partial_clients.items():
        info = client_info_table.get(cid, {})
        st_rs = info.get("short_term_rs", 0.0)
        lt_rs = info.get("long_term_rs", 0.0)
        
        combined_weight = beta * lt_rs + (1 - beta) * st_rs
        total_weight += combined_weight
        
        if weighted_params is None:
            weighted_params = [combined_weight * p for p in params]
        else:
            for idx, p in enumerate(params):
                weighted_params[idx] += combined_weight * p
    
    if total_weight <= 1e-9:
        # è‹¥ç™¼ç¾åŠ æ¬Šå¾Œå¹¾ä¹ç‚º0ï¼Œä»£è¡¨æ‰€æœ‰å®¢æˆ¶ç«¯çš„è²è­½éƒ½å¾ˆä½(æˆ–æŸäº›é‚è¼¯éŒ¯èª¤)
        print("[Warning] re_aggregate_without: total_weight=0 => fallback returns None")
        return None
    
    aggregated = [p / total_weight for p in weighted_params]
    return aggregated

def evaluate_model(params: List[np.ndarray], test_loader: DataLoader) -> float:
    net = Net().to(DEVICE)
    set_parameters(net, params)
    loss, _ = test(net, test_loader)
    return float(loss)

def calculate_shapley(
    global_model: List[np.ndarray],
    client_models: Dict[str, List[np.ndarray]],
    test_loader: DataLoader,
    client_info_table: Dict[str, Dict[str, float]],
    beta: float = 0.7
) -> Dict[str, float]:
    """
    ä»¥Leave-one-outçš„æ–¹å¼è¿‘ä¼¼å„å®¢æˆ¶ç«¯çš„Shapleyå€¼:
      1) å…ˆè¨ˆç®—åŒ…å«å…¨éƒ¨å®¢æˆ¶ç«¯çš„èšåˆæ¨¡å‹(ä¹Ÿå¯ç›´æ¥ç”¨ global_model åƒè€ƒå€¼)
      2) åˆ†åˆ¥ã€Œç§»é™¤cidã€å¾Œé‡æ–°èšåˆä¸€æ¬¡åš evaluate
      3) Shapley = |(no-cid-model-loss) - (base_loss)|
    """
    shapley_values = {}
    
    # 0) æº–å‚™ base_loss
    #    å¦‚æœæ‚¨ç¢ºå®š global_model å°±æ˜¯ã€ŒåŒ…å«æ‰€æœ‰å®¢æˆ¶ç«¯èšåˆã€å¾Œçš„çµæœï¼Œå¯ç›´æ¥ç”¨å®ƒ
    base_loss = evaluate_model(global_model, test_loader)
    
    # 1) æº–å‚™ä¸€ä»½ã€Œæ‰€æœ‰ client åƒæ•¸ã€åŠ æ¬Šèšåˆ => fallback_base_model
    #    è‹¥æ‚¨å¸Œæœ›è·Ÿ global_model ä¸€æ¨¡ä¸€æ¨£ï¼Œå¯ç›´æ¥ç•¥éé€™ä¸€æ­¥ï¼Œç”¨ global_model å³å¯
    #    ä¸éä¹Ÿå¯ä»¥ç”¨ "client_models" åšä¸€æ¬¡ sanitizeã€‚
    all_cleaned = {}
    for cid, p in client_models.items():
        all_cleaned[cid] = check_and_clean_params(p, cid)
    # é‡æ–°åŠ æ¬Šèšåˆ
    all_aggregated = re_aggregate_without(all_cleaned, exclude_cid=None, 
                                          client_info_table=client_info_table, 
                                          beta=beta)
    if all_aggregated is None:
        # è¡¨ç¤ºå¯èƒ½åªæœ‰1å€‹client => fallback
        all_aggregated = global_model
    
    # 2) ç”¨è©²å…¨é‡èšåˆæ¨¡å‹çš„ loss ç•¶ base_loss (ä¹Ÿå¯ç›´æ¥ç”¨ global_model çš„ evaluate)
    fallback_base_loss = evaluate_model(all_aggregated, test_loader)
    
    # 3) é€å€‹ client åšã€Œleave-one-out èšåˆã€å† evaluate
    for cid in client_models:
        # è·³éåƒæ•¸ç©ºçš„
        if not client_models[cid]:
            shapley_values[cid] = 0.0
            continue
        
        # (a) é‡æ–°èšåˆ (æ’é™¤è©²cid)
        temp_aggregated = re_aggregate_without(all_cleaned, cid, client_info_table, beta=beta)
        if temp_aggregated is None:
            # è¡¨ç¤ºç§»é™¤å¾Œæ²’æœ‰åƒèˆ‡è€… => shapley=base_loss - ??? => å¯è‡ªè¡Œå®šç¾©ç‚º0
            shapley_values[cid] = 0.0
            continue
        
        # (b) è©•ä¼° temp_loss
        temp_loss = evaluate_model(temp_aggregated, test_loader)
        
        # (c) Shapley = |(no-cid-loss) - (all-loss)|
        shap = abs(temp_loss - fallback_base_loss)
        shapley_values[cid] = shap
    
    return shapley_values

def update_client_info_table(
    client_info_table: Dict[str, Dict[str, float]],
    fit_results: List[Tuple[ClientProxy, FitRes]],
    global_model: List[np.ndarray],  # æ–°å¢åƒæ•¸
    alpha: float = 0.5,
) -> None:
    """
    æ ¹æ“šæœ¬è¼ªçš„è¨“ç·´çµæœ (loss, metrics...) æ›´æ–° client_info_tableã€‚
    é™¤äº†æ›´æ–° loss èˆ‡ reputationï¼Œä¹Ÿæ›´æ–° similarityï¼ˆçœ‹ä½œæ˜¯æ…¢é€Ÿæ›´æ–°èˆ‡å…¨å±€åƒæ•¸é–“çš„ç›¸ä¼¼æ€§ï¼‰ã€‚
    """
    # print("[INFO] global_model :", global_model)
    _, _, test_loader = get_cached_datasets(0)
    trained_cids = {client_proxy.cid for (client_proxy, _) in fit_results}
    # print('[INFO] trained_cids:', trained_cids)
    trained_partitions = {
        fit_res.metrics.get("partition_id", client_proxy.cid)
        for (client_proxy, fit_res) in fit_results
    }
    print('[INFO] trained_partitions:', trained_partitions)
    
    # (ä¿®æ­£) æº–å‚™ç•¶å‰ client_models çµ¦ Shapley è¨ˆç®—
    client_models_dict = {}
    for cp, fit_res in fit_results:
        cid = cp.cid
        
        # æ¸…ç† NaN/Inf
        raw_params = parameters_to_ndarrays(fit_res.parameters)
        raw_params = check_and_clean_params(raw_params, cid)
        client_models_dict[cid] = raw_params
    
    # (ä¿®æ­£) ç”¨ leave-one-out æ–¹å¼è¨ˆç®— Shapley
    shapley_values = calculate_shapley(
        global_model=global_model,
        client_models=client_models_dict,
        test_loader=test_loader,
        client_info_table=client_info_table,
        beta=0.7  # æ‚¨è‡ªå®šç¾©
    )
    # print("[INFO] shapley_values:", shapley_values)
    
    # å¯«å…¥ client_info_table
    for cid, shap_val in shapley_values.items():
        client_info_table[cid]["shapley"] = shap_val
        hist = client_info_table[cid].get("shapley_history", [])
        hist.append(shap_val)
        if len(hist) > 5:
            hist.pop(0)
        client_info_table[cid]["shapley_history"] = hist

    # 3) æ›´æ–°å…¶é¤˜è³‡è¨Š (loss, accuracy, similarity, short_term_rs...)
    for (client_proxy, fit_res) in fit_results:
        cid = client_proxy.cid
        fit_metrics = fit_res.metrics

        # (a) åŸºæœ¬è³‡è¨Š
        # client_info_table[cid]["partition_id"] = fit_metrics["partition_id"]
        maybe_pid = fit_metrics.get("partition_id", -1)
        if maybe_pid != -1:
            client_info_table[cid]["partition_id"] = maybe_pid
        latest_loss = float(fit_metrics.get("client_loss", 9999.9))
        latest_acc = float(fit_metrics.get("client_accuracy", 0.0))
        client_info_table[cid]["client_loss"] = latest_loss
        client_info_table[cid]["client_accuracy"] = latest_acc

        # (b) ç¶­è­· loss_history
        loss_history = client_info_table[cid].get("loss_history", [])
        if latest_loss < 9999.9:
            loss_history.append(latest_loss)
        if len(loss_history) > 5:
            loss_history.pop(0)
        client_info_table[cid]["loss_history"] = loss_history

        # (c) ç¶­è­· accuracy_historyï¼ˆæ–°å¢ï¼‰
        accuracy_history = client_info_table[cid].get("accuracy_history", [])
        accuracy_history.append(latest_acc)
        if len(accuracy_history) > 10:  # å‡è¨­é•·æœŸè¦çœ‹æœ€è¿‘10è¼ª
            accuracy_history.pop(0)
        client_info_table[cid]["accuracy_history"] = accuracy_history

        # (d) è¨ˆç®—èˆ‡ global params çš„ similarity
        client_params_nd = parameters_to_ndarrays(fit_res.parameters)
        
        if global_model is not None:
            #è¨ˆç®—similarity
            similarity = _compute_similarity_compressed(global_model, client_params_nd)
            # print(f"[INFO] Client {cid} similarity: {similarity:.4f}")
            client_info_table[cid]["similarity"] = similarity
            sim_history = client_info_table[cid].get("similarity_history", [])
            sim_history.append(similarity)
            if len(sim_history) > 5:
                sim_history.pop(0)
            client_info_table[cid]["similarity_history"] = sim_history

        # (e) è¨ˆç®—çŸ­æœŸRS (èˆ‡åŸç¨‹å¼ç›¸åŒ)
        loss_std = np.std(loss_history) if len(loss_history) >= 2 else 0
        similarity_now = client_info_table[cid]["similarity"]

        short_term_rs = (
            0.6 * latest_acc +
            0.2 * (1 - loss_std) +
            0.2 * max(0, similarity_now)
        )
        client_info_table[cid]["short_term_rs"] = short_term_rs

    # === 4) è¨ˆç®—ã€Œæ­¥é©Ÿä¸‰ï¼šé•·æœŸ RS (LongTermScore)ã€ ===
    #    - é•·æœŸè¡¨ç¾ (Performance) = accuracy_history çš„å¹³å‡
    #    - é•·æœŸå¯é åº¦ (Reliability) = 1 - std(accuracy_history)ï¼ˆæˆ–å…¶ä»–å®šç¾©ï¼‰
    #    - é•·æœŸShapley (Shapley) = shapley_history çš„å¹³å‡
    #    - æœ€å¾Œæ•´åˆï¼š LongTermScore = w3*Performance + w4*Reliability + w5*Shapley

    w3, w4, w5 = 0.4, 0.3, 0.3  # æ¬Šé‡å¯è‡ªè¡Œèª¿æ•´
    for cid in trained_cids:
        info = client_info_table[cid]
        # (a) å–å¾—æ­·å²
        acc_hist = info.get("accuracy_history", [])
        shap_hist = info.get("shapley_history", [])

        # (b) é•·æœŸè¡¨ç¾ Performance = å¹³å‡ accuracy
        if len(acc_hist) > 0:
            longterm_performance = float(np.mean(acc_hist))
        else:
            longterm_performance = 0.0

        # (c) é•·æœŸå¯é åº¦ Reliability = 1 - std(accuracy)ï¼Œé¿å…è² å€¼å‰‡å– max(0, 1 - std)
        if len(acc_hist) >= 2:
            acc_std = float(np.std(acc_hist))
        else:
            acc_std = 0.0
        longterm_reliability = max(0.0, 1.0 - acc_std)

        # (d) é•·æœŸ Shapley = shapley_history å¹³å‡
        if len(shap_hist) > 0:
            longterm_shapley = float(np.mean(shap_hist))
        else:
            longterm_shapley = 0.0

        # (e) çµåˆæˆ LongTermScore
        long_term_score = (
            w3 * longterm_performance +
            w4 * longterm_reliability +
            w5 * longterm_shapley
        )
        client_info_table[cid]["long_term_rs"] = long_term_score

    # (E) **æ­¥é©Ÿå››**ï¼šå°‡çŸ­æœŸ RS èˆ‡é•·æœŸ RS æ•´åˆæˆæœ€çµ‚ Reputation
    #    1) é¿å…äº’ç›¸å£“åˆ¶ï¼Œå¯ç”¨å–®ç´”åŠ æ¬Šå¹³å‡ï¼šRS_raw = Î± * ST + (1-Î±) * LT
    #    2) åš reliability tuning æˆ–æ•¸å€¼è£åˆ‡
    #    3) è‹¥å·®ç•°éå¤§ï¼Œå¯åšæ‡²ç½°æˆ–è·³é

    Î± = 0.5  # æ‚¨å¯è‡ªè¡Œæ±ºå®šæ¬Šé‡
    for cid in trained_cids:
        st_rs = client_info_table[cid]["short_term_rs"]
        lt_rs = client_info_table[cid]["long_term_rs"]

        # (1) åŠ æ¬Š
        rs_raw = Î± * st_rs + (1 - Î±) * lt_rs

        # (2) å¯é¸ï¼šè‹¥çŸ­æœŸèˆ‡é•·æœŸå·®ç•°éå¤§ => æ‡²ç½°æˆ–èª¿æ•´
        diff_ratio = abs(st_rs - lt_rs) / (lt_rs + 1e-8)
        if diff_ratio > 0.8:
            # èˆ‰ä¾‹ï¼šè‹¥å·®ç•° > 0.8ï¼Œå°‡æœ€çµ‚å€¼ä¹˜ä»¥ 0.9 æ‡²ç½°
            rs_raw = rs_raw * 0.9

        # (3) æ•¸å€¼è£åˆ‡ (é¿å…éå¤§æˆ–å°)
        rs_final = max(0.0, min(1.0, rs_raw))  # ä»‹æ–¼ 0~1

        # å­˜åˆ° reputation
        client_info_table[cid]["reputation"] = rs_final

def sanitize_aggregation(client_params_list: Dict[str, List[np.ndarray]],
                         client_info_table: Dict[str, Dict[str, float]]) -> List[np.ndarray]:
    if not client_params_list:
        print("[Warning] No valid client parameters to aggregate. Returning None.")
        return None

    weighted_params = None
    total_weight = 0.0
    for cid, params in client_params_list.items():
        info = client_info_table.get(cid, {})
        rep_score = info.get("reputation", 1.0)  # æ­¥é©Ÿå››å¾Œçš„æœ€çµ‚ reputation

        total_weight += rep_score
        if weighted_params is None:
            weighted_params = [rep_score * p for p in params]
        else:
            for idx, p in enumerate(params):
                weighted_params[idx] += rep_score * p

    if total_weight < 1e-9:
        return None

    aggregated_ndarrays = [p / total_weight for p in weighted_params]
    return aggregated_ndarrays

def apply_time_decay(client_info_table: Dict[str, Dict[str, float]], 
                     decay_factor: float = 0.95):
    """åœ¨æ¯è¼ªé–‹å§‹å‰å°é•·æœŸRSæ–½åŠ æ™‚é–“è¡°æ¸›"""
    for cid in client_info_table:
        lt_rs = client_info_table[cid].get("long_term_rs", 1.0)
        client_info_table[cid]["long_term_rs"] = decay_factor * lt_rs

def detect_anomalies(client_info_table: Dict[str, Dict[str, float]], threshold: float = 0.3):
    for cid, info in client_info_table.items():
        rep_score = info.get("reputation", 0.0)
        # ä¾‹å¦‚ï¼šè‹¥ reputation < 0.1ï¼Œç›´æ¥è¦–ç‚ºå¯ç–‘
        if rep_score < threshold:
            info["is_suspicious"] = True
        else:
            info["is_suspicious"] = False

def median_aggregation(client_params_list: Dict[str, List[np.ndarray]]) -> List[np.ndarray]:
    """
    å°æ¯ä¸€å±¤çš„åƒæ•¸åš element-wise medianã€‚
    å›å‚³èšåˆå¾Œçš„åƒæ•¸ï¼ˆList[np.ndarray]ï¼‰ï¼Œç¶­åº¦èˆ‡å–®ä¸€ client çš„ params ç›¸åŒã€‚
    """
    # å…ˆæŠŠæ‰€æœ‰å®¢æˆ¶ç«¯çš„åƒæ•¸æ•´ç†æˆ list of list
    # shape: n_clients x n_layers x (param_shape...)
    all_params = list(client_params_list.values())
    n_clients = len(all_params)
    if n_clients == 0:
        return []
    
    n_layers = len(all_params[0])  # å‡è¨­æ‰€æœ‰ client çš„ layer æ•¸ç›¸åŒ
    aggregated = []
    
    for layer_idx in range(n_layers):
        # æ”¶é›†æ‰€æœ‰ client çš„åŒä¸€å±¤åƒæ•¸
        layer_stack = np.array([all_params[c][layer_idx] for c in range(n_clients)])
        # layer_stack shape = (n_clients, ...) ä¾‹å¦‚ (n_clients, 128, 256)
        
        # è¨ˆç®—è©²å±¤æ¯å€‹ element çš„ median => axis=0
        layer_median = np.median(layer_stack, axis=0)
        aggregated.append(layer_median)
    
    return aggregated

def trimmed_mean_aggregation(
    client_params_list: Dict[str, List[np.ndarray]],
    trim_ratio: float = 0.1
) -> List[np.ndarray]:
    """
    å°æ¯ä¸€å±¤çš„åƒæ•¸åš element-wise trimmed meanã€‚
    trim_ratio=0.1 ä»£è¡¨æ¯å€‹ä½ç½®æœƒåˆªé™¤å‰10%å’Œå¾Œ10%çš„æ•¸å€¼ï¼Œå†å°å‰©é¤˜80%åšå¹³å‡ã€‚
    """
    all_params = list(client_params_list.values())
    n_clients = len(all_params)
    if n_clients == 0:
        return []
    
    n_layers = len(all_params[0])
    aggregated = []
    
    # è¨ˆç®—è¦åˆªé™¤çš„æ•¸é‡
    k = int(n_clients * trim_ratio)
    
    for layer_idx in range(n_layers):
        layer_stack = np.array([all_params[c][layer_idx] for c in range(n_clients)])
        # shape = (n_clients, ...)
        
        # å°æ¯å€‹ element é€²è¡Œæ’åº => éœ€è¦å…ˆæ”¤å¹³æˆ–ç”¨ np.sort(..., axis=0)
        # åšæ³•1ï¼šå…ˆ reshape => (n_clients, -1) => æ’åº => å–ä¸­é–“ => reshapeå›åŸå½¢
        shape_original = layer_stack[0].shape
        layer_2d = layer_stack.reshape(n_clients, -1)  # => (n_clients, n_params)
        # é‡å°æ¯å€‹ column (å°æ‡‰ä¸€å€‹ element) æ’åº
        layer_2d_sorted = np.sort(layer_2d, axis=0)
        
        # åˆªé™¤å‰ k èˆ‡å¾Œ k
        # æ³¨æ„è‹¥ n_clients < 2k => æœƒå‡ºéŒ¯ï¼Œéœ€äº‹å…ˆæª¢æŸ¥
        valid_slice = layer_2d_sorted[k : n_clients - k, :]  # shape => ((n_clients-2k), n_params)
        
        # å–å¹³å‡
        trimmed_mean_1d = np.mean(valid_slice, axis=0)
        
        # reshape å›åŸæœ¬å½¢ç‹€
        layer_trimmed_mean = trimmed_mean_1d.reshape(shape_original)
        aggregated.append(layer_trimmed_mean)
    
    return aggregated

class MyFedAvgWithDynamic(FedAvg):
    """
    åœ¨ MyFedAvg åŸºç¤ä¸Šï¼ŒåŠ å…¥ã€å‹•æ…‹å®¢æˆ¶ç«¯ç®¡ç†ã€çš„åŠŸèƒ½ï¼Œä¸¦ä½¿ç”¨ DataFrame å„²å­˜å®¢æˆ¶ç«¯è³‡è¨Šã€‚
    """
    def __init__(self,
                 fraction_fit,
                 alpha=0.5,
                 aggregation_mode="robust",  # æ–°å¢
                 robust_method="median",     # "median" / "trimmed_mean" / "krum" / ...
                 **kwargs):
        super().__init__(**kwargs)
        self.fraction_fit = fraction_fit  # æƒ³è¦æ¯å›åˆé¸æ“‡å¤šå°‘æ¯”ä¾‹çš„å®¢æˆ¶ç«¯
        self.alpha = alpha  # ç”¨æ–¼æ»‘å‹•å¹³å‡
        self.aggregation_mode = aggregation_mode
        self.robust_method = robust_method
        self.client_info_table: Dict[str, Dict[str, float]] = {}  # è¨˜éŒ„å®¢æˆ¶ç«¯è³‡è¨Š
        self.client_info_df = pd.DataFrame()  # å­˜å„²å®¢æˆ¶ç«¯è³‡è¨Šçš„ DataFrame
        self.previous_global_params = None  # ç”¨ä¾†ä¿å­˜ä¸Šä¸€è¼ªåˆ†ç™¼çµ¦å®¢æˆ¶ç«¯çš„å…¨å±€åƒæ•¸
        self.metric_history: Dict[str, Dict[str, List[float]]] = {}

    # def initialize_parameters(
    #     self, client_manager: ClientManager
    # ) -> Optional[Parameters]:
    #     """Initialize global model parameters."""
    #     net = Net()
    #     ndarrays = get_parameters(net)
    #     return ndarrays_to_parameters(ndarrays)
    
    def initialize_parameters(
        self, client_manager: ClientManager
    ) -> Optional[Parameters]:
        """Initialize global model parameters."""
        initial_parameters = self.initial_parameters
        self.initial_parameters = None  # Don't keep initial parameters in memory
        return initial_parameters

    def configure_fit(
        self,
        server_round: int,
        parameters: Parameters,
        client_manager: flwr.server.client_manager.ClientManager,
    ) -> List[Tuple[ClientProxy, flwr.common.FitIns]]:
        # åœ¨æ¯è¼ªé–‹å§‹æ™‚å…ˆåšæ™‚é–“è¡°æ¸›
        apply_time_decay(self.client_info_table, decay_factor=0.95)
        # è¨˜éŒ„ç•¶å‰å›åˆç™¼å‡ºçš„å…¨å±€åƒæ•¸ï¼ˆç”¨ä¾†åš similarity æ¯”è¼ƒï¼‰
        self.previous_global_params = parameters_to_ndarrays(parameters)
        old_fraction = self.fraction_fit  # æš«å­˜èˆŠå€¼

        config = self.on_fit_config_fn(server_round) if self.on_fit_config_fn else {}
        fit_ins = FitIns(parameters, config)
    
        all_clients = list(client_manager.all().values())
        if server_round <= 1:
            self.fraction_fit = 1.0
            fit_ins = super().configure_fit(server_round, parameters, client_manager)
            # åˆå§‹åŒ–è¡¨æ ¼è³‡è¨Šï¼ˆè‹¥è©²clienté‚„æ²’æœ‰è¨˜éŒ„ï¼‰
            all_client_ids = [c[0].cid for c in fit_ins]
            init_or_check_client_info_table(self.client_info_table, all_client_ids)
            self.fraction_fit = old_fraction
            return fit_ins
        
        # å…ˆä¾ç…§ cid æ’åºï¼Œç¢ºä¿çµæœå›ºå®š
        sorted_clients = sorted(all_clients,
                                key=lambda x: self.client_info_table[x.cid]["reputation"],  # æ”¹ç”¨é•·æœŸRSæ’åº
                                reverse=True,)
        
        # è¨ˆç®—è¦é¸å–çš„å®¢æˆ¶ç«¯æ•¸é‡ (ä¾‹å¦‚10å€‹å®¢æˆ¶ç«¯ï¼Œfraction_fit=0.5ï¼Œå‰‡é¸å– int(10*0.5) = 5 å€‹)
        num_clients_to_select = int(len(sorted_clients) * self.fraction_fit)
        
        # é¸å–å‰é¢ num_clients_to_select å€‹å®¢æˆ¶ç«¯
        selected_clients = sorted_clients[:num_clients_to_select]
        
        return [(client, fit_ins) for client in selected_clients]

    def configure_evaluate(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> List[Tuple[ClientProxy, EvaluateIns]]:
        """Configure the next round of evaluation."""
        if self.fraction_evaluate == 0.0:
            return []
        config = {}
        evaluate_ins = EvaluateIns(parameters, config)

        # Sample clients
        sample_size, min_num_clients = self.num_evaluation_clients(
            client_manager.num_available()
        )
        clients = client_manager.sample(
            num_clients=sample_size, min_num_clients=min_num_clients
        )

        # Return client/config pairs
        return [(client, evaluate_ins) for client in clients]

    def aggregate_fit(
        self,
        server_round: int,
        fit_results: List[Tuple[ClientProxy, FitRes]],
        failures: List[BaseException],
    ) -> Optional[Tuple[Parameters, Dict[str, Scalar]]]:
    
    	# 1) æ”¶é›†æ‰€æœ‰ client çš„åƒæ•¸
        client_params_list = {}

        for client_proxy, fit_res in fit_results:
            cid = client_proxy.cid
            client_params_nd = parameters_to_ndarrays(fit_res.parameters)
            client_params_list[cid] = client_params_nd

        # 2) æ›´æ–° client_info_table
        update_client_info_table(self.client_info_table, fit_results, self.previous_global_params, alpha=self.alpha)

        # æ›´æ–° DataFrame ä¸¦å°å‡º
        self.client_info_df = pd.DataFrame.from_dict(self.client_info_table, orient='index')
        # print(f"\nğŸ“Š [Round {server_round} Client Info Table] ğŸ“Š")
        # display(self.client_info_df)
        
        # ===== æ–°å¢ï¼šè¨˜éŒ„æœ¬è¼ªå„ client çš„æŒ‡æ¨™ =====
        for cid, info in self.client_info_table.items():
            partition_id = info.get("partition_id", cid)  # ç”¨ partition_id ç•¶ä½œè­˜åˆ¥
            # è‹¥è©² partition_id å°šæœªåœ¨ metric_history ä¸­ï¼Œå…ˆåˆå§‹åŒ–
            if partition_id not in self.metric_history:
                self.metric_history[partition_id] = {
                    "reputation": [],
                    "short_term_rs": [],
                    "long_term_rs": [],
                    "similarity": [],
                    "shapley": [],
                }
            # å°‡æœ¬è¼ªçš„æŒ‡æ¨™è¨˜éŒ„ä¸‹ä¾†ï¼ˆè‹¥è©² client æœ¬è¼ªæœªåƒèˆ‡ï¼Œå‰‡ client_info_table ä»ä¿ç•™ä¸Šä¸€è¼ªå€¼ï¼‰
            self.metric_history[partition_id]["reputation"].append(info.get("reputation", 0.0))
            self.metric_history[partition_id]["short_term_rs"].append(info.get("short_term_rs", 0.0))
            self.metric_history[partition_id]["long_term_rs"].append(info.get("long_term_rs", 0.0))
            self.metric_history[partition_id]["similarity"].append(info.get("similarity", 0.0))
            self.metric_history[partition_id]["shapley"].append(info.get("shapley", 0.0))
        # ==========================================

        # Debug: ç¢ºä¿ `client_info_table` å·²æ›´æ–°
        print(f"[DEBUG] Updated client_info_table (Round {server_round}):")

        #ç•°å¸¸æª¢æ¸¬ï¼Œæœƒç›´æ¥è·³éç•°å¸¸å®¢æˆ¶ç«¯(è¦æ”¹)
        # detect_anomalies(self.client_info_table, threshold=0.6)

        # 3) ä¾ç…§ aggregation_mode æ±ºå®šè¦ç”¨å“ªä¸€ç¨®èšåˆæ–¹å¼
        if self.aggregation_mode == "robust":
            # (a) é€²ä¸€æ­¥åˆ†æ”¯ robust_method: "median", "trimmed_mean", "krum", "rfa", ...
            if self.robust_method == "median":
                aggregated_ndarrays = median_aggregation(client_params_list)
            elif self.robust_method == "trimmed_mean":
                aggregated_ndarrays = trimmed_mean_aggregation(client_params_list, trim_ratio=0.1)
            else:
                # å…¶ä»– robust æ–¹æ³•...
                aggregated_ndarrays = median_aggregation(client_params_list)  # é è¨­ç•¶ median
	
        elif self.aggregation_mode == "score":
            # ç”¨ä½ å…ˆå‰çš„ Weighted/Score-based æ–¹æ¡ˆ
            # å¯èƒ½ä¹Ÿè¦å…ˆç¯©æ‰ is_suspicious çš„ client
            sanitized_params = {
                cid: p for cid, p in client_params_list.items()
                if not self.client_info_table[cid]["is_suspicious"]
            }
            aggregated_ndarrays = sanitize_aggregation(sanitized_params, self.client_info_table)
	
        else:
            # å¦‚æœéƒ½ä¸æ˜¯ => fallback ç”¨é è¨­ FedAvg
            return super().aggregate_fit(server_round, fit_results, failures)

		# 4) è‹¥ aggregated_ndarrays = Noneï¼Œfallback ç”¨ super
        if aggregated_ndarrays is None:
            print("[Warning] aggregator returned None. Fallback to FedAvg.")
            return super().aggregate_fit(server_round, fit_results, failures)
	
        # 5) æ›´æ–° self.previous_global_params
        self.previous_global_params = aggregated_ndarrays
	
        # 6) è½‰å›æˆ Flower éœ€è¦çš„å‹æ…‹ (Parameters)
        aggregated_parameters = ndarrays_to_parameters(aggregated_ndarrays)

        # 7) å›å‚³
        return aggregated_parameters, {}

    def aggregate_evaluate(
        self,
        server_round: int,
        results: List[Tuple[ClientProxy, EvaluateRes]],
        failures: List[BaseException],
    ) -> Optional[float]:
        
        losses = []
        accuracies = []
        # å°‡æ¯å€‹ client å›å‚³çš„ metrics è¨˜éŒ„åˆ° history
        for client_proxy, evaluate_res in results:
            cid = client_proxy.cid
            if cid not in self.client_info_table:
                init_or_check_client_info_table(self.client_info_table, [cid])

            metrics = evaluate_res.metrics  # e.g. {"loss": 0.12, "accuracy": 0.87, ...}
            # print(metrics)
            if "client_loss" in metrics:
                loss_val = metrics["client_loss"]
                losses.append(loss_val)
                self.client_info_table[cid]["client_loss"] = loss_val
                # å°‡é€™å€‹ loss è¨˜éŒ„åˆ°åˆ†æ•£å¼ (distributed) çš„ loss
                history.add_loss_distributed(server_round, loss_val)
            if "client_accuracy" in metrics:
                acc_val = metrics["client_accuracy"]
                accuracies.append(acc_val)
                self.client_info_table[cid]["client_accuracy"] = acc_val
                # å°‡é€™å€‹ accuracy è¨˜éŒ„åˆ°åˆ†æ•£å¼ (distributed) çš„ metrics
                history.add_metrics_distributed(server_round, {"accuracy": acc_val})
            
            loss_history = self.client_info_table[cid].get("loss_history", [])
            if(loss_val < 9999.9):
                loss_history.append(loss_val)
            if len(loss_history) > 5:  # ä¿ç•™æœ€è¿‘5è¼ªæ•¸æ“š
                loss_history.pop(0)
            self.client_info_table[cid]["loss_history"] = loss_history
        
        # å…ˆå‘¼å«çˆ¶é¡åˆ¥ï¼Œå–å¾—é è¨­çš„èšåˆçµæœ(é€šå¸¸æ˜¯å¹³å‡loss)
        aggregated_loss = super().aggregate_evaluate(server_round, results, failures)
        aggregated_accuracy = sum(accuracies) / len(accuracies) if accuracies else 0.0
        
        return aggregated_loss, {"accuracy": aggregated_accuracy}

    def evaluate(
        self, server_round: int, parameters: Parameters
    ) -> Optional[tuple[float, dict[str, Scalar]]]:
        """Evaluate model parameters using an evaluation function."""
        if self.evaluate_fn is None:
            # No evaluation function provided
            return None
        parameters_ndarrays = parameters_to_ndarrays(parameters)
        eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
        if eval_res is None:
            return None
        loss, metrics = eval_res
        return loss, metrics

    def num_fit_clients(self, num_available_clients: int) -> Tuple[int, int]:
        """Return sample size and required number of clients."""
        num_clients = int(num_available_clients * self.fraction_fit)
        return max(num_clients, self.min_fit_clients), self.min_available_clients

    def num_evaluation_clients(self, num_available_clients: int) -> Tuple[int, int]:
        """Use a fraction of available clients for evaluation."""
        num_clients = int(num_available_clients * self.fraction_evaluate)
        return max(num_clients, self.min_evaluate_clients), self.min_available_clients
